#data processing

You should treat your data format as 'sample.txt', formatted as 'label \t query\t document_txt'.In detail, label donotes the relation between query and document, 1 means the query is related to the document, otherwise it does not matter. The words in query and documents are separated by white space. To understand that most models require 'corpus_preprocessed.txt', 'relation_train.txt',  'relation_valid.txt', 'relation_test.txt', 'embed_glove_d50' files, do the following:

1.Generate the files named 'relation_train.txt', 'relation_valid.txt', 'relation_test.txt' and 'corpus.txt' by executing the file named  'preparation.py' which can be found under 'MatchZoo/matchzoo/inputs'. You can follow the process of data production below, which is the function 'main' of the 'preparation.py' script.

## beginning of this process
1)create Preparation object.
2)specify the base directory where you want to load the files.
3)call the function 'run_with_one_corpus' of the Preparation object and specify a parameter, which denotes the raw data. The function  'run_with_one_corpus' transfers the format of 'sample.txt' to the format like this 'id words', then outputs the relation files 'rel' between queries and documents.
4)save 'corpus.txt' by calling the function 'save_corpus' of Preparation object.
5)shuffle the relationship in the file 'rel', and then cut it into a specified percentage of the file. Detailedly, if you want to adjust output data radio, specify the second parameter of the function 'split_train_valid_test', which represent the percentage of the training data, validation data and test data, orderly.
6)save the relationship file by calling the function 'save_relation' of Preparation object, specifying the path where you want to save. 
## end of this process


2.Generate the files named 'corpus_preprocessed.txt' , 'word_dict.txt'. And if you need CF,CF,IDF of documents, you can save 'word_stats.txt' by executing the function 'preprocessor.save_words_stats'. The models in MatchZoo requires that all words must be id instead of string, so you should map the string to id by executing the function named 'preprocessor.run', then specify it's output name and save it. To generate the files, reference to the function 'main' of the file 'preprocess.py' which can be found under MatchZoo/matchzoo/inputs.

## beginning of this process
1)create Preprocess object, and you can specify the parameter to make some constraint like specifying the frequency of words, in which case words will be filtered if they are not in the frequency band etc.
2)execute the function 'run' of object Preprocess, then get the documents' id and words mapped as id, with your inititialization parameters.
3)save the word dict file by executing function 'save_word_dict' of Preprocess object, and save 'word_stats.txt' by executing the function 'save_words_stats' of Preprocess object too, which contains information like DF,CF,IDF sequentially.
4)then save corpus information whose words has been mapped as id to the path specified by yourself.
## end of this process


3.Generate the file named 'embed_glove_d50' by executing the file named 'gen_w2v.py' which is located under MatchZoo/data/WikiQA/, with three parameters 'glove.840B.300d.txt', 'word_dict.txt', 'embed_glove_d300'. The first parameter denotes the embedding downloaded from the url 'http://nlp.stanford.edu/data/glove.840B.300d.zip' or where you want; the second parameter denotes the file mapping string words to words id; the last parameter denotes the output embedding for your own dataset starting with word id and following with embeddings.

## begining of this process
1)load word dict file and word vectors sequentially.
2)get the dimension of embedding you downloaded.
3)get the third parameter where is the path you want to output.
4)write the word embedding in your corpus.
5)randomly generated it, if the words in your corpus are not in the embedding file you downloaded. 
## end of this process


4.What's more, here is how to generate special files for other models:

	4.1 Generate the files that DRMM needs. Generate the IDF file by cutting off a part of 'word_stats.txt' with this command 'cat word_stats.txt | cut -d ' ' -f 1,4 > embed.idf' in linux console. Then generate histograms of DRMM. You can run this script 'python gen_hist4drmm.py 60' as WiKiQA data which is under MatchZoo/data/WikiQA, and the only parameter represents the size of histograms. Follow the steps found in function 'main' at file 'gen_hist4drmm.py', you can generate this file.

	## beginning of this process
	1) get the size of histograms first.
	2) specify the source file, and then define the path of the embedding file, relation file(including train, valid, and test), output file for histograms.
	3) randomly generate embedding vectors, then cover downloaded embedding into it. The purpose of doing so is to make words that are not in the dict also own random embedding to be used.
	4) read the corpus.
	5) generate histograms by calling the function 'cal_hist' in the file 'preprocess', which can be found in MatchZoo/matchzoo/inputs, then write it.
	## end of this process


	4.2 Generate the files that ANMM needs by executing 'gen_binsum4anmm.py' with the only parameter that represents the number of bin, And you can found it under MatchZoo/data/WikiQA as well. It is similar to generating histograms for DRMM, and the only difference is that the function 'cal_hist' above is replaced by the function 'cal_binsum'.
	
	4.3 Generate the files that DSSM and CDSSM require. You can refer to the code after line 66 of the file called 'prepare_mz_data.py' which is under the folder 'MatchZoo/data/WikiQA/'. The following steps generate triletter of words in corpus.
	
	## beginning of this process
	1)define the input file 'word_dict.txt', output file 'triletter_dict.txt' and 'word_triletter_map.txt'. In detail, the file 'triletter_dict.txt' contains information representing trilletter of words and its id, orderly. What's more, the file 'word_triletter_map.txt' denotes words id and the id of the triletter it decomposed.
	2)read word dict.
	3)specify the begining and end symbol of the current word.
	4)split it to trilletter. Call the function 'NgramUtil.ngrams' whose first parameter denotes the word to be splited, the second is how big granularity the word will be splited, and the last is the symbol of the connection string.
	5)fillter the too low and too high frequency words in the dict by calling the function 'filter_triletter' and setting the minimum and maximum values.
	## end of this process


